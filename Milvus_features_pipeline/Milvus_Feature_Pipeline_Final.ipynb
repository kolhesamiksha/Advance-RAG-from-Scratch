{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \"semantic-router[fastembed]\" langchain langchain_community==0.2.6 fastembed==0.3.2 langchain_core openai pymilvus bs4 \"grpcio<=1.63.0,>=1.49.1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU semantic-chunkers==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from langchain_core.documents import Document\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from openai import OpenAI\n",
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection, connections, utility\n",
    "from typing import Any, List, Tuple, Dict, Literal, Optional\n",
    "from pydantic import Field\n",
    "from semantic_router.schema import DocumentSplit\n",
    "from langchain_core.documents import Document\n",
    "from semantic_router.splitters import RollingWindowSplitter\n",
    "from semantic_router.utils.logger import logger\n",
    "from semantic_router.encoders import FastEmbedEncoder\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Credentials\n",
    "ZILLIZ_CLOUD_URI = \"\"         # os.environ['ZILLIZ_URI'] #\"https://in03-c2cc7c5da8decab.api.gcp-us-west1.zillizcloud.com\"\n",
    "ZILLIZ_CLOUD_API_KEY = \"\"\n",
    "COLLECTION_NAME=\"trial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE_EMBEDDING_MODEL = \"\"\n",
    "SPARSE_EMBEDDING_MODEL = \"\"\n",
    "SEMANTIC_ENCODER = \"\"\n",
    "SEMANTIC_SCORE_THERESHOLD = 0.3\n",
    "BASE_URL = 'https://docs.nvidia.com/cuda/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Milvus connection and collection status First\n",
    "def connection_status(collection_name):\n",
    "    connections.connect(\n",
    "        uri=ZILLIZ_CLOUD_URI,\n",
    "        token=ZILLIZ_CLOUD_API_KEY\n",
    "    )\n",
    "\n",
    "    utility.get_server_version()\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "    else:\n",
    "        print(f\"New Collection -> {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from pydantic import BaseModel, Field, model_validator, validator\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class SparseFastEmbedEmbeddings(BaseModel, Embeddings):\n",
    "    \"\"\"Qdrant FastEmbedding models.\n",
    "    FastEmbed is a lightweight, fast, Python library built for embedding generation.\n",
    "\n",
    "    To use this class, you must install the `fastembed` Python package.\n",
    "\n",
    "    `pip install fastembed`\n",
    "    Example:\n",
    "        from langchain_community.embeddings import FastEmbedEmbeddings\n",
    "        fastembed = FastEmbedEmbeddings()\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = \"BAAI/bge-small-en-v1.5\"\n",
    "    \"\"\"Name of the FastEmbedding model to use\n",
    "    Defaults to \"BAAI/bge-small-en-v1.5\"\n",
    "    Find the list of supported models at\n",
    "    https://qdrant.github.io/fastembed/examples/Supported_Models/\n",
    "    \"\"\"\n",
    "\n",
    "    cache_dir: Optional[str] = Field(default=None)\n",
    "    \"\"\"The path to the cache directory.\n",
    "    Defaults to `local_cache` in the parent directory\n",
    "    \"\"\"\n",
    "\n",
    "    threads: Optional[int] = Field(default=None)\n",
    "    \"\"\"The number of threads single onnxruntime session can use.\n",
    "    Defaults to None\n",
    "    \"\"\"\n",
    "\n",
    "    doc_embed_type: str = \"default\"\n",
    "    \"\"\"Type of embedding to use for documents\n",
    "    The available options are: \"default\" and \"passage\"\n",
    "    \"\"\"\n",
    "\n",
    "    model: Any = Field(default=None, exclude=True)  # Renamed to 'model' and marked as private\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        extra = 'forbid'\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that FastEmbed has been installed.\"\"\"\n",
    "        return values\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize the FastEmbed model.\"\"\"\n",
    "        try:\n",
    "            # >= v0.2.0\n",
    "            from fastembed import SparseTextEmbedding\n",
    "\n",
    "            self.model = SparseTextEmbedding(\n",
    "                model_name=self.model_name,\n",
    "                cache_dir=self.cache_dir,\n",
    "                threads=self.threads,\n",
    "            )\n",
    "        except ImportError as ie:\n",
    "            try:\n",
    "                # < v0.2.0\n",
    "                from fastembed.embedding import FlagEmbedding\n",
    "\n",
    "                self.model = FlagEmbedding(\n",
    "                    model_name=self.model_name,\n",
    "                    cache_dir=self.cache_dir,\n",
    "                    threads=self.threads,\n",
    "                )\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"Could not import 'fastembed' Python package. \"\n",
    "                    \"Please install it with `pip install fastembed`.\"\n",
    "                ) from ie\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for documents using FastEmbed.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        embeddings: List[np.ndarray]\n",
    "        if self.doc_embed_type == \"passage\":\n",
    "            embeddings = self.model.passage_embed(texts)\n",
    "        else:\n",
    "            embeddings = self.model.embed(texts)\n",
    "        return [\n",
    "            {int(idx): float(val) for idx, val in zip(embed.indices, embed.values)}\n",
    "            for embed in embeddings\n",
    "        ]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate query embeddings using FastEmbed.\n",
    "\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "\n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        query_embeddings: np.ndarray = next(self.model.query_embed(text))\n",
    "        return query_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intelligent WebCrawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_links(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  main_links = []\n",
    "\n",
    "  # Extracting links with \"index.html\" considering nesting\n",
    "  for a in soup.find_all('a', href=True):\n",
    "    href = a['href']\n",
    "    if href and href.endswith(\"index.html\"):  # Check for ending with \"index.html\"\n",
    "      full_url = urljoin(url, href)\n",
    "      # Avoid duplicate links and links pointing to external domains\n",
    "      if full_url not in main_links and full_url.startswith(url):\n",
    "        main_links.append(full_url)\n",
    "\n",
    "  return main_links\n",
    "\n",
    "# Function to extract subsection links from a main link\n",
    "def get_subsection_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    subsection_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        full_url = urljoin(url, href)\n",
    "        if '#' in full_url and full_url not in subsection_links:\n",
    "            subsection_links.append(full_url)\n",
    "    return subsection_links\n",
    "\n",
    "# Function to extract the main content by section ID and its subsections\n",
    "def extract_section_content(cached_soup, section_id):\n",
    "  section_content = []\n",
    "  soup = cached_soup  # Use the cached soup object\n",
    "\n",
    "  main_section = soup.find('section', {'id': section_id})\n",
    "\n",
    "  if main_section:\n",
    "    for element in main_section.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'pre']):\n",
    "            if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                section_content.append('\\n' + element.get_text().upper() + '\\n')\n",
    "            elif element.name == 'p':\n",
    "                section_content.append(element.get_text() + '\\n\\n')\n",
    "            elif element.name == 'li':\n",
    "                section_content.append('* ' + element.get_text() + '\\n')\n",
    "            elif element.name == 'pre':  # for code blocks\n",
    "                section_content.append('\\n' + element.get_text() + '\\n')\n",
    "\n",
    "  return '\\n'.join(section_content)\n",
    "\n",
    "def extract_main_section_ids(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "  # Find all sections with an ID attribute\n",
    "  sections_with_id = soup.find_all('section', {'id': True})\n",
    "\n",
    "  # This list will store top-level section IDs (no nested sections)\n",
    "  main_section_ids = []\n",
    "\n",
    "  # Iterate through sections with ID\n",
    "  for section in sections_with_id:\n",
    "    # Check if the current section has a parent section with an ID\n",
    "    if not section.parent or not section.parent.has_attr('id'):\n",
    "      # If no parent ID, it's likely a top-level section\n",
    "      main_section_ids.append(section['id'])\n",
    "\n",
    "  return main_section_ids, soup\n",
    "\n",
    "def create_document(content, main_link, section_id):\n",
    "  doc_data = Document(page_content=content, metadata = {\"source_link\":main_link, \"section_id\":section_id})\n",
    "  return doc_data\n",
    "\n",
    "# Main function to orchestrate the extraction and saving process\n",
    "def Scrape_data(base_url):\n",
    "    base_url = base_url\n",
    "    main_document_list = []\n",
    "\n",
    "    # Step 1: Extract main links\n",
    "    main_links = get_main_links(base_url)\n",
    "    \n",
    "    # Step 2: Extract subsection links and save content\n",
    "    for main_link in main_links:\n",
    "        # Extract main section IDs dynamically\n",
    "        main_section_ids, soup = extract_main_section_ids(main_link)\n",
    "        # Extract and save content grouped by main sections\n",
    "        for section_id in main_section_ids:\n",
    "            if section_id!=\"notices\":\n",
    "              content = extract_section_content(soup, section_id)\n",
    "              if content:\n",
    "                  sub_doc = create_document(content, main_link, section_id)\n",
    "                  main_document_list.append(sub_doc)\n",
    "\n",
    "    return main_document_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_dim(model_name):\n",
    "    embeddings = FastEmbedEmbeddings(model_name=model_name)\n",
    "    document_embeddings = embeddings.embed_documents(\"Have a great day\")\n",
    "    return len(document_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(collection_name, model_name):\n",
    "    dimension = get_embedding_dim(model_name)\n",
    "    fields = [\n",
    "        FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"source_link\", dtype=DataType.VARCHAR, max_length=500),\n",
    "        FieldSchema(\n",
    "            name=\"text\", dtype=DataType.VARCHAR, max_length=65535\n",
    "        ),\n",
    "        FieldSchema(\n",
    "            name=\"prechunk\", dtype=DataType.ARRAY, element_type=DataType.VARCHAR, max_capacity=5, max_length=65535\n",
    "            ),\n",
    "        FieldSchema(\n",
    "            name=\"postchunk\", dtype=DataType.ARRAY, element_type=DataType.VARCHAR, max_capacity=5, max_length=65535\n",
    "            ),\n",
    "        FieldSchema(\n",
    "            name=\"section_id\", dtype=DataType.VARCHAR, max_length=65535\n",
    "        ),\n",
    "        FieldSchema(name=\"sparse_vector\", dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "        FieldSchema(name=\"dense_vector\", dtype=DataType.FLOAT_VECTOR, dim=dimension),\n",
    "    ]\n",
    "\n",
    "    schema = CollectionSchema(fields=fields)\n",
    "    collection = Collection(name=collection_name, schema=schema, shards_num=1, consistency_level=\"Strong\")\n",
    "\n",
    "    dense_index_params = {\n",
    "        \"index_type\": \"IVF_SQ8\",\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"params\": {\"nlist\": 128},\n",
    "    }\n",
    "\n",
    "    sparse_index_params = {\n",
    "        \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
    "        \"metric_type\": \"IP\",\n",
    "    }\n",
    "    collection.create_index(field_name=\"sparse_vector\", index_params=sparse_index_params)\n",
    "    collection.create_index(field_name=\"dense_vector\", index_params=dense_index_params)\n",
    "    collection.load()\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_encode_docs(texts: List[str], embed_model):\n",
    "    embeddings = FastEmbedEmbeddings(model_name=embed_model)\n",
    "    document_embeddings = embeddings.embed_documents(texts)\n",
    "    return document_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_encode_docs(texts: List[str], embed_model):\n",
    "    embeddings = SparseFastEmbedEmbeddings(model_name=embed_model)\n",
    "    document_embeddings = embeddings.embed_documents(texts)\n",
    "    return document_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_insert(data: list, collection, sparse_embed_model, dense_embed_model):\n",
    "    print(data[1])\n",
    "    sparse_embeddings = sparse_encode_docs(data[1], sparse_embed_model)\n",
    "    dense_embeddings = dense_encode_docs(data[1], dense_embed_model)\n",
    "    collection.insert(\n",
    "        [\n",
    "            data[0], # source            \n",
    "            data[1], # text, page_content\n",
    "            data[2], # prechunk\n",
    "            data[3], # postchunk\n",
    "            data[4], #section_id\n",
    "            sparse_embeddings, # sparse_embedding\n",
    "            dense_embeddings, # dense_embeddings\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metadata(doc_metadata:dict, splits: list[DocumentSplit]):\n",
    "    source_link_meta = doc_metadata['source_link']\n",
    "    section_id_meta = doc_metadata['section_id']\n",
    "    final_doc = []\n",
    "    for i, split in enumerate(splits):\n",
    "        prechunk = [splits[i-2].content if i-2 >= 0 else \"\", splits[i-1].content if i-1 >= 0 else \"\"]\n",
    "        postchunk = [splits[i+1].content if i+1 < len(splits) else \"\", splits[i+2].content if i+2 < len(splits) else \"\"]\n",
    "        page_content = split.content \n",
    "        metadata = { \n",
    "            \"prechunk\": prechunk,\n",
    "            \"postchunk\": postchunk,\n",
    "            \"source_link\": source_link_meta,\n",
    "            \"section_id\" : section_id_meta\n",
    "        }\n",
    "        doc_obj = Document(page_content=page_content, metadata=metadata)\n",
    "        final_doc.append(doc_obj)\n",
    "\n",
    "    return final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_splitter(encoder_name, semantic_score_threshold):\n",
    "    encoder = FastEmbedEncoder(name=encoder_name)\n",
    "\n",
    "    encoder.score_threshold = semantic_score_threshold\n",
    "\n",
    "    splitter = RollingWindowSplitter(\n",
    "        encoder=encoder,\n",
    "        dynamic_threshold=False,\n",
    "        min_split_tokens=100,\n",
    "        max_split_tokens=2000,\n",
    "        window_size=3,\n",
    "        plot_splits=False,  # set this to true to visualize chunking\n",
    "        enable_statistics=True  # to print chunking stats\n",
    "    )\n",
    "    return splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_db(prepared_data, collection_obj, collection_name, sparse_embed_model, dense_embed_model):\n",
    "    data_batch = [[], [], [], [], []]\n",
    "    overall_time_st = time.time()\n",
    "    BATCH_SIZE = 2\n",
    "    current_source = None\n",
    "    section = 0\n",
    "    current_title = None\n",
    "    doc_title = None\n",
    "\n",
    "    for content in prepared_data:\n",
    "        source = content.metadata[\"source_link\"]\n",
    "        page_content = content.page_content\n",
    "        prechunk = content.metadata[\"prechunk\"]\n",
    "        postchunk = content.metadata[\"postchunk\"]\n",
    "        section = content.metadata[\"section_id\"]\n",
    "\n",
    "        data_batch[0].append(source)\n",
    "        data_batch[1].append(page_content)\n",
    "        data_batch[2].append(prechunk)\n",
    "        data_batch[3].append(postchunk)\n",
    "        data_batch[4].append(section)\n",
    "        \n",
    "        if len(data_batch[0]) % BATCH_SIZE == 0:\n",
    "            print(\"Inside Data embed\")\n",
    "            st = time.time()\n",
    "            ins = embed_insert(data_batch, collection_obj, sparse_embed_model, dense_embed_model)\n",
    "            print(\"Total time taken to  process each batch & insert data to milvus is: \", time.time() - st)\n",
    "            data_batch = [[], [], [], [], []]\n",
    "            print(\"=\"*100)\n",
    "            # break\n",
    "\n",
    "    print(\"overall time to prepare data for insertion: \", time.time() - overall_time_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Store_data_Milvus():\n",
    "\n",
    "    #Check the Connection status with Milvus\n",
    "    connection_status(COLLECTION_NAME)\n",
    "\n",
    "    # Create a collection\n",
    "    collection = create_collection(COLLECTION_NAME, DENSE_EMBEDDING_MODEL)\n",
    "    print(\"Successfully Loaded the Collection\")\n",
    "    \n",
    "    # Scrape the Base-URL Data\n",
    "    scraped_data_lst = Scrape_data(BASE_URL)\n",
    "\n",
    "    # Prepare Data\n",
    "    splitter = semantic_splitter(SEMANTIC_ENCODER, SEMANTIC_SCORE_THERESHOLD)\n",
    "\n",
    "    # Prepare Metadata and Store in Milvus\n",
    "    for data in scraped_data_lst[:3]:\n",
    "        splits = splitter([data.page_content])\n",
    "        doc_metadata = data.metadata\n",
    "        prepared_data = build_metadata(doc_metadata, splits)\n",
    "        insert_data_db(prepared_data, collection, COLLECTION_NAME, SPARSE_EMBEDDING_MODEL, DENSE_EMBEDDING_MODEL)\n",
    "    return {\"Successfully Store Data Into the Milvus\"}, collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 62415.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded the Collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 77385.68it/s]\n",
      "\u001b[32m2024-07-16 13:50:10 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 2000. Splitting to sentences before semantically splitting.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 449\n",
      "  - Total Splits: 3\n",
      "  - Splits by Threshold: 0\n",
      "  - Splits by Max Chunk Size: 2\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 197\n",
      "  - Maximum Token Size of Split: 1996\n",
      "  - Similarity Split Ratio: 0.00\n",
      "Inside Data embed\n",
      "['1. CUDA 12.5 UPDATE 1 RELEASE NOTES\\uf0c1 The release notes for the NVIDIA® CUDA® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html. Note The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases. 1.1. CUDA TOOLKIT MAJOR COMPONENT VERSIONS\\uf0c1 Starting with CUDA 11, the various components in the toolkit are versioned independently. For CUDA 12.5 Update 1, the table below indicates the versions: Component Name Version Information Supported Architectures Supported Platforms CUDA C++ Core Compute Libraries Thrust 2.4.0 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUB 2.4.0 libcu++ 2.4.0 Cooperative Groups 12.5.82 CUDA Compatibility 12.5.36505571 aarch64-jetson Linux CUDA Runtime (cudart) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuobjdump 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUPTI 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuxxfilt (demangler) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA Demo Suite 12.5.82 x86_64 Linux, Windows CUDA GDB 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, WSL CUDA Nsight Eclipse Plugin 12.5.82 x86_64 Linux CUDA NVCC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvdisasm 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA NVML Headers 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvprof 12.5.82 x86_64 Linux, Windows CUDA nvprune 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVRTC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL NVTX 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVVP 12.5.82 x86_64, Linux, Windows CUDA OpenCL 12.5.39 x86_64 Linux, Windows CUDA Profiler API 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA Compute Sanitizer API 12.5.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuBLAS 12.5.3.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuDLA 12.5.82 aarch64-jetson Linux CUDA cuFFT 11.2.3.61 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuFile 1.10.1.7 x86_64, arm64-sbsa, aarch64-jetson Linux CUDA cuRAND 10.3.6.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSOLVER 11.6.3.83 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSPARSE 12.5.1.3 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NPP 12.3.0.159 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvFatbin 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJitLink 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJPEG 12.3.2.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL Nsight Compute 2024.2.1.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL (Windows 11) Nsight Systems 2024.2.3.38 x86_64, arm64-sbsa, Linux, Windows, WSL Nsight Visual Studio Edition (VSE) 2024.2.1.24155 x86_64 (Windows) Windows nvidia_fs1 2.20.6 x86_64, arm64-sbsa, aarch64-jetson Linux Visual Studio Integration 12.5.82 x86_64 (Windows) Windows NVIDIA Linux Driver 555.42.06 x86_64, arm64-sbsa Linux NVIDIA Windows Driver 555.85 x86_64 (Windows) Windows, WSL Running a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3. For more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus. Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases. More information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades. Note: Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below. The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html CUDA Toolkit Minimum Required Driver Version for CUDA Minor Version Compatibility* Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.x >=525.60.13 >=528.33 CUDA 11.8.x CUDA 11.7.x CUDA 11.6.x CUDA 11.5.x CUDA 11.4.x CUDA 11.3.x CUDA 11.2.x CUDA 11.1.x >=450.80.02 >=452.39 CUDA 11.0 (11.0.3) >=450.36.06** >=451.22** * Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode – please read the CUDA Compatibility Guide for details. ** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits. The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below. CUDA Toolkit Toolkit Driver Version Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.5 Update 1 >=555.42.06 >=555.85 CUDA 12.5 GA >=555.42.02 >=555.85 CUDA 12.4 Update 1 >=550.54.15 >=551.78 CUDA 12.4 GA >=550.54.14 >=551.61 CUDA 12.3 Update 1 >=545.23.08 >=546.12 CUDA 12.3 GA >=545.23.06 >=545.84 CUDA 12.2 Update 2 >=535.104.05 >=537.13 CUDA 12.2 Update 1 >=535.86.09 >=536.67 CUDA 12.2 GA >=535.54.03 >=536.25 CUDA 12.1 Update 1 >=530.30.02 >=531.14 CUDA 12.1 GA >=530.30.02 >=531.14 CUDA 12.0 Update 1 >=525.85.12 >=528.33 CUDA 12.0 GA >=525.60.13 >=527.41 CUDA 11.8 GA >=520.61.05 >=520.06 CUDA 11.7 Update 1 >=515.48.07 >=516.31 CUDA 11.7 GA >=515.43.04 >=516.01 CUDA 11.6 Update 2 >=510.47.03 >=511.65 CUDA 11.6 Update 1 >=510.47.03 >=511.65 CUDA 11.6 GA >=510.39.01 >=511.23 CUDA 11.5 Update 2 >=495.29.05 >=496.13', 'CUDA 11.5 Update 1 >=495.29.05 >=496.13 CUDA 11.5 GA >=495.29.05 >=496.04 CUDA 11.4 Update 4 >=470.82.01 >=472.50 CUDA 11.4 Update 3 >=470.82.01 >=472.50 CUDA 11.4 Update 2 >=470.57.02 >=471.41 CUDA 11.4 Update 1 >=470.57.02 >=471.41 CUDA 11.4.0 GA >=470.42.01 >=471.11 CUDA 11.3.1 Update 1 >=465.19.01 >=465.89 CUDA 11.3.0 GA >=465.19.01 >=465.89 CUDA 11.2.2 Update 2 >=460.32.03 >=461.33 CUDA 11.2.1 Update 1 >=460.32.03 >=461.09 CUDA 11.2.0 GA >=460.27.03 >=460.82 CUDA 11.1.1 Update 1 >=455.32 >=456.81 CUDA 11.1 GA >=455.23 >=456.38 CUDA 11.0.3 Update 1 >= 450.51.06 >= 451.82 CUDA 11.0.2 GA >= 450.51.05 >= 451.48 CUDA 11.0.1 RC >= 450.36.06 >= 451.22 CUDA 10.2.89 >= 440.33 >= 441.22 CUDA 10.1 (10.1.105 general release, and updates) >= 418.39 >= 418.96 CUDA 10.0.130 >= 410.48 >= 411.31 CUDA 9.2 (9.2.148 Update 1) >= 396.37 >= 398.26 CUDA 9.2 (9.2.88) >= 396.26 >= 397.44 CUDA 9.1 (9.1.85) >= 390.46 >= 391.29 CUDA 9.0 (9.0.76) >= 384.81 >= 385.54 CUDA 8.0 (8.0.61 GA2) >= 375.26 >= 376.51 CUDA 8.0 (8.0.44) >= 367.48 >= 369.30 CUDA 7.5 (7.5.16) >= 352.31 >= 353.66 CUDA 7.0 (7.0.28) >= 346.46 >= 347.62 For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs. For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers. During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages). For more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software. For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas. 1.2. NEW FEATURES\\uf0c1 This section lists new general CUDA and CUDA compilers features. 1.2.1. GENERAL CUDA\\uf0c1 * In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option. End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules. In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option. End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules. * MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here. MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here. 1.2.2. CUDA COMPILER\\uf0c1 * For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5. For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5. 1.2.3. CUDA DEVELOPER TOOLS\\uf0c1 * For changes to nvprof and Visual Profiler, see the changelog. For changes to nvprof and Visual Profiler, see the changelog. * For new features, improvements, and bug fixes in Nsight Systems, see the changelog. For new features, improvements, and bug fixes in Nsight Systems, see the changelog. * For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog. For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog. * For new features, improvements, and bug fixes in CUPTI, see the changelog. For new features, improvements, and bug fixes in CUPTI, see the changelog. * For new features, improvements, and bug fixes in Nsight Compute, see the changelog. For new features, improvements, and bug fixes in Nsight Compute, see the changelog. * For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog. For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog. * For new features, improvements, and bug fixes in CUDA-GDB, see the changelog. For new features, improvements, and bug fixes in CUDA-GDB, see the changelog. 1.3. RESOLVED ISSUES\\uf0c1 1.3.1. CUDA COMPILER\\uf0c1 * Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device. Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device. * Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler. Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler. * Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag. Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag. * Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops. Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops. * Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to “Misaligned shared or local address”. Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to “Misaligned shared or local address”. * Fix to correct the calculation of write-after-read hazard latency. Fix to correct the calculation of write-after-read hazard latency. 1.4. KNOWN ISSUES AND LIMITATIONS\\uf0c1 * Runfile will not be supported for Amazon Linux 2023. Runfile will not be supported for Amazon Linux 2023. * Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features. Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features. * Launching Cooperative Group kernels with MPS is not supported on Tegra platforms. Launching Cooperative Group kernels with MPS is not supported on Tegra platforms. 1.5. DEPRECATED OR DROPPED FEATURES\\uf0c1 Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software. 1.5.1. DEPRECATED OR DROPPED ARCHITECTURES\\uf0c1 * NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5. NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5. 1.5.2. DEPRECATED OPERATING SYSTEMS\\uf0c1 * NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5. NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5. * CUDA 12.5 is the last release to support Debian 10.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 91180.52it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 67432.54it/s]\n",
      "\u001b[32m2024-07-16 13:50:37 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 2000. Splitting to sentences before semantically splitting.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken to  process each batch & insert data to milvus is:  4.527934551239014\n",
      "====================================================================================================\n",
      "overall time to prepare data for insertion:  4.528050899505615\n",
      "Splitting Statistics:\n",
      "  - Total Documents: 1204\n",
      "  - Total Splits: 12\n",
      "  - Splits by Threshold: 0\n",
      "  - Splits by Max Chunk Size: 11\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 1107\n",
      "  - Maximum Token Size of Split: 2000\n",
      "  - Similarity Split Ratio: 0.00\n",
      "Inside Data embed\n",
      "['2. CUDA LIBRARIES\\uf0c1 This section covers CUDA Libraries release notes for 12.x releases. * CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host. CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host. 2.1. CUBLAS LIBRARY\\uf0c1 2.1.1. CUBLAS: RELEASE 12.5 UPDATE 1\\uf0c1 * New Features Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs. New Features * Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs. Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs. * Known Issues The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release. cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release. Known Issues * The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release. The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release. * cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release. cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release. * Resolved Issues Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error. cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D). Resolved Issues * Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error. Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error. * cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D). cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D). 2.1.2. CUBLAS: RELEASE 12.5\\uf0c1 * New Features cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs. This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details. New Features * cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs. This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details. cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs. This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details. * Known Issues cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types. Known Issues * cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types. cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types. * Resolved Issues cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results. cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv. Resolved Issues * cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results. cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results. * cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv. cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv. 2.1.3. CUBLAS: RELEASE 12.4 UPDATE 1\\uf0c1 * Known Issues Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail. This will be fixed in an upcoming release. cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release. Known Issues * Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail. This will be fixed in an upcoming release. Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail. This will be fixed in an upcoming release. * cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release. cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release. * Resolved Issues cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2). Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(), cublasLtMatmulAlgoCheck(), and cublasLtMatmulAlgoGetHeuristic(). The issue was introduced in CUDA Toolkit 12.4. cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8. Resolved Issues', '* cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2). cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2). * Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(), cublasLtMatmulAlgoCheck(), and cublasLtMatmulAlgoGetHeuristic(). The issue was introduced in CUDA Toolkit 12.4. Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(), cublasLtMatmulAlgoCheck(), and cublasLtMatmulAlgoGetHeuristic(). The issue was introduced in CUDA Toolkit 12.4. * cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8. cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8. 2.1.4. CUBLAS: RELEASE 12.4\\uf0c1 * New Features cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision. Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH. Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta). Please see gemmGroupedBatched for more details. New Features * cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision. Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH. Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta). Please see gemmGroupedBatched for more details. cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision. Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH. Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta). Please see gemmGroupedBatched for more details. * Known Issues When the current context has been created using cuGreenCtxCreate(), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget(). BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6. When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. Known Issues * When the current context has been created using cuGreenCtxCreate(), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget(). When the current context has been created using cuGreenCtxCreate(), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget(). * BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1. BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1. * cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1. * cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6. * When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. 2.1.5. CUBLAS: RELEASE 12.3 UPDATE 1\\uf0c1 * New Features Improved performance of heuristics cache for workloads that have a high eviction rate. New Features * Improved performance of heuristics cache for workloads that have a high eviction rate. Improved performance of heuristics cache for workloads that have a high eviction rate. * Known Issues BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST. Known Issues * BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 86778.70it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 54189.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken to  process each batch & insert data to milvus is:  4.476426601409912\n",
      "====================================================================================================\n",
      "Inside Data embed\n",
      "['If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST. BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST. * Resolved Issues cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute(). Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS). cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient. Resolved Issues * cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. * When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute(). When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute(). * Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS). Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS). * cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient. 2.1.6. CUBLAS: RELEASE 12.3\\uf0c1 * New Features Improved performance on NVIDIA L40S Ada GPUs. New Features * Improved performance on NVIDIA L40S Ada GPUs. Improved performance on NVIDIA L40S Ada GPUs. * Known Issues cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute(). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit(). This will be fixed in an upcoming release. Known Issues * cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. * When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute(). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit(). This will be fixed in an upcoming release. When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute(). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit(). This will be fixed in an upcoming release. 2.1.7. CUBLAS: RELEASE 12.2 UPDATE 2\\uf0c1 * New Features cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel. It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times. This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable. New Features * cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel. It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times. This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable. cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel. It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times. This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable. * Known Issues cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. Known Issues * cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. 2.1.8. CUBLAS: RELEASE 12.2\\uf0c1 * Known Issues cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%.', 'There is currently no workaround for this issue. Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE. The kernels apply the first batch’s bias vector to all batches. This will be fixed in a future release. Known Issues * cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue. cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue. * Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE. The kernels apply the first batch’s bias vector to all batches. This will be fixed in a future release. Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE. The kernels apply the first batch’s bias vector to all batches. This will be fixed in a future release. 2.1.9. CUBLAS: RELEASE 12.1 UPDATE 1\\uf0c1 * New Features Support for FP8 on NVIDIA Ada GPUs. Improved performance on NVIDIA L4 Ada GPUs. Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions. New Features * Support for FP8 on NVIDIA Ada GPUs. Support for FP8 on NVIDIA Ada GPUs. * Improved performance on NVIDIA L4 Ada GPUs. Improved performance on NVIDIA L4 Ada GPUs. * Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions. Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions. * Known Issues When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes. Known Issues * When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes. When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes. 2.1.10. CUBLAS: RELEASE 12.0 UPDATE 1\\uf0c1 * New Features Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs. New Features * Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs. Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs. * Known Issues For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture. Known Issues * For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture. For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture. * Resolved Issues Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache. This began in the CUDA Toolkit 12.0 release. Added forward compatible single precision complex GEMM that does not require workspace. Resolved Issues * Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache. This began in the CUDA Toolkit 12.0 release. Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache. This began in the CUDA Toolkit 12.0 release. * Added forward compatible single precision complex GEMM that does not require workspace. Added forward compatible single precision complex GEMM that does not require workspace. 2.1.11. CUBLAS: RELEASE 12.0\\uf0c1 * New Features cublasLtMatmul now supports FP8 with a non-zero beta. Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface. Added more Hopper-specific kernels for cublasLtMatmul with epilogues: CUBLASLT_EPILOGUE_BGRAD{A,B} CUBLASLT_EPILOGUE_{RELU,GELU}_AUX CUBLASLT_EPILOGUE_D{RELU,GELU} Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux. New Features * cublasLtMatmul now supports FP8 with a non-zero beta. cublasLtMatmul now supports FP8 with a non-zero beta. * Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface. Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface. * Added more Hopper-specific kernels for cublasLtMatmul with epilogues: CUBLASLT_EPILOGUE_BGRAD{A,B} CUBLASLT_EPILOGUE_{RELU,GELU}_AUX CUBLASLT_EPILOGUE_D{RELU,GELU} Added more Hopper-specific kernels for cublasLtMatmul with epilogues: * CUBLASLT_EPILOGUE_BGRAD{A,B} CUBLASLT_EPILOGUE_BGRAD{A,B} * CUBLASLT_EPILOGUE_{RELU,GELU}_AUX']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 89240.51it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 74631.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken to  process each batch & insert data to milvus is:  3.0369365215301514\n",
      "====================================================================================================\n",
      "Inside Data embed\n",
      "['CUBLASLT_EPILOGUE_{RELU,GELU}_AUX * CUBLASLT_EPILOGUE_D{RELU,GELU} CUBLASLT_EPILOGUE_D{RELU,GELU} * Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux. Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux. * Known Issues There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release. Known Issues * There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release. There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release. * Resolved Issues Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient. cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues. Deprecations Disallow including cublas.h and cublas_v2.h in the same translation unit. Removed: CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore. cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t. CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t. CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed. Resolved Issues * Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient. Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient. * cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues. cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues. Deprecations * Disallow including cublas.h and cublas_v2.h in the same translation unit. Disallow including cublas.h and cublas_v2.h in the same translation unit. * Removed: CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore. cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t. CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t. CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed. Removed: * CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore. CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore. * cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t. cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t. * CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t. CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t. * CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed. CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed. 2.2. CUFFT LIBRARY\\uf0c1 2.2.1. CUFFT: RELEASE 12.5\\uf0c1 * New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes. We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API. New Features * Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes. We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API. Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes. * We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API. We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API. 2.2.2. CUFFT: RELEASE 12.4 UPDATE 1\\uf0c1 * Resolved Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header. Resolved Issues * A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header. A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header. 2.2.3. CUFFT: RELEASE 12.4\\uf0c1 * New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing. Added per-plan properties to the cuFFT API.', 'These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs. Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes. New Features * Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing. Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing. * Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs. Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs. * Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes. Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes. * Known Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release. Known Issues * A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release. A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release. * Resolved Issues Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API). Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension. Resolved Issues * Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API). Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API). * Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension. Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension. 2.2.4. CUFFT: RELEASE 12.3 UPDATE 1\\uf0c1 * Known Issues Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT. Known Issues * Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT. Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT. * Resolved Issues Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context. Resolved Issues * Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context. Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context. 2.2.5. CUFFT: RELEASE 12.3\\uf0c1 * New Features Callback kernels are more relaxed in terms of resource usage, and will use fewer registers. Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127. Slightly improved planning times for some FFT sizes. New Features * Callback kernels are more relaxed in terms of resource usage, and will use fewer registers. Callback kernels are more relaxed in terms of resource usage, and will use fewer registers. * Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127. Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127. * Slightly improved planning times for some FFT sizes. Slightly improved planning times for some FFT sizes. 2.2.6. CUFFT: RELEASE 12.2\\uf0c1 * New Features cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs. Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT. Reduced the size of the static libraries when compared to cuFFT in the 12.1 release. New Features * cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs. cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs. * Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT. Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT. * Reduced the size of the static libraries when compared to cuFFT in the 12.1 release. Reduced the size of the static libraries when compared to cuFFT in the 12.1 release. * Resolved Issues cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive. cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently. Resolved Issues * cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive. cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive. * cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently. cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently. 2.2.7. CUFFT: RELEASE 12.1 UPDATE 1\\uf0c1 * Known Issues cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023. cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans. Known Issues * cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023. cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023. * cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans. cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans. 2.2.8. CUFFT: RELEASE 12.1\\uf0c1 * New Features']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 69905.07it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 79739.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken to  process each batch & insert data to milvus is:  2.838268995285034\n",
      "====================================================================================================\n",
      "Inside Data embed\n",
      "[\"Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout. New Features * Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout. Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout. * Known Issues Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4. Known Issues * Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4. Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4. * Resolved Issues cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit. Resolved Issues * cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit. cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit. 2.2.9. CUFFT: RELEASE 12.0 UPDATE 1\\uf0c1 * Resolved Issues Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced. Resolved Issues * Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced. Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced. 2.2.10. CUFFT: RELEASE 12.0\\uf0c1 * New Features PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures. New Features * PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures. PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures. * Known Issues cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme. Known Issues * cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme. cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme. * Resolved Issues cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved. Resolved Issues * cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved. cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved. 2.3. CUSOLVER LIBRARY\\uf0c1 2.3.1. CUSOLVER: RELEASE 12.5 UPDATE 1\\uf0c1 * Resolved Issues The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved. Resolved Issues * The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved. The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved. 2.3.2. CUSOLVER: RELEASE 12.5\\uf0c1 * New Features Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'. Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR. Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices. New Features * Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'. Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'. * Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR. Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR. * Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices. Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices. * Known Issues With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with auto ALIGN_32=[](int64_t val) { return ((val + 31)/32)*32; }; and auto sizeofCudaDataType=[](cudaDataType dt) { if (dt == CUDA_R_32F) return sizeof(float); if (dt == CUDA_R_64F) return sizeof(double); if (dt == CUDA_C_32F) return sizeof(cuComplex); if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex); }; Known Issues * With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with auto ALIGN_32=[](int64_t val) { return ((val + 31)/32)*32; }; and auto sizeofCudaDataType=[](cudaDataType dt) { if (dt == CUDA_R_32F) return sizeof(float); if (dt == CUDA_R_64F) return sizeof(double); if (dt == CUDA_C_32F) return sizeof(cuComplex); if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex); }; With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with auto ALIGN_32=[](int64_t val) { return ((val + 31)/32)*32; }; and auto sizeofCudaDataType=[](cudaDataType dt) { if (dt == CUDA_R_32F) return sizeof(float); if (dt == CUDA_R_64F) return sizeof(double); if (dt == CUDA_C_32F) return sizeof(cuComplex); if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex); }; 2.3.3. CUSOLVER: RELEASE 12.4 UPDATE 1\\uf0c1 * New Features The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd. The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster. New Features * The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd. The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd.\", '* The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster. The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster. * Resolved Issues cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes. Resolved Issues * cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes. cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes. * Deprecations Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cusolverDnGesvd,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnXsyevdx, cusolverDnXgesvd, and the corresponding bufferSize functions instead. Deprecations * Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cusolverDnGesvd,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnXsyevdx, cusolverDnXgesvd, and the corresponding bufferSize functions instead. Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cusolverDnGesvd,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnXsyevdx, cusolverDnXgesvd, and the corresponding bufferSize functions instead. 2.3.4. CUSOLVER: RELEASE 12.4\\uf0c1 * New Features cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes. New Features * cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes. cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes. * Known Issues cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size. Known Issues * cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size. cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size. 2.3.5. CUSOLVER: RELEASE 12.2 UPDATE 2\\uf0c1 * Resolved Issues Fixed an issue with cusolverDn<t>gesvd(), cusolverDnGesvd(), and cusolverDnXgesvd(), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to ‘N’. Resolved Issues * Fixed an issue with cusolverDn<t>gesvd(), cusolverDnGesvd(), and cusolverDnXgesvd(), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to ‘N’. Fixed an issue with cusolverDn<t>gesvd(), cusolverDnGesvd(), and cusolverDnXgesvd(), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to ‘N’. 2.3.6. CUSOLVER: RELEASE 12.2\\uf0c1 * New Features A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode(). Affected functions are: cusolverDn<t>geqrf(), cusolverDn<t>syevd(), cusolverDn<t>syevdx(), cusolverDn<t>gesvdj(), cusolverDnXgeqrf(), cusolverDnXsyevd(), cusolverDnXsyevdx(), cusolverDnXgesvdr(), and cusolverDnXgesvdp(). New Features * A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode(). Affected functions are: cusolverDn<t>geqrf(), cusolverDn<t>syevd(), cusolverDn<t>syevdx(), cusolverDn<t>gesvdj(), cusolverDnXgeqrf(), cusolverDnXsyevd(), cusolverDnXsyevdx(), cusolverDnXgesvdr(), and cusolverDnXgesvdp(). A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode(). Affected functions are: cusolverDn<t>geqrf(), cusolverDn<t>syevd(), cusolverDn<t>syevdx(), cusolverDn<t>gesvdj(), cusolverDnXgeqrf(), cusolverDnXsyevd(), cusolverDnXsyevdx(), cusolverDnXgesvdr(), and cusolverDnXgesvdp(). * Known Issues Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock. Known Issues * Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock. Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock. 2.4. CUSPARSE LIBRARY\\uf0c1 2.4.1. CUSPARSE: RELEASE 12.5 UPDATE 1\\uf0c1 * New Features Added support for BSR format in cusparseSpMM. New Features * Added support for BSR format in cusparseSpMM. Added support for BSR format in cusparseSpMM. * Resolved Issues cusparseSpMM() would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches. cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1). cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \\\\*= beta. The bug behavior was not modifying C at all. cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows. Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 91512.09it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 79739.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken to  process each batch & insert data to milvus is:  3.5387067794799805\n",
      "====================================================================================================\n",
      "Inside Data embed\n",
      "['Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes. Resolved Issues * cusparseSpMM() would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches. cusparseSpMM() would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches. * cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1). cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1). * cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \\\\*= beta. The bug behavior was not modifying C at all. cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \\\\*= beta. The bug behavior was not modifying C at all. * cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows. cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows. * Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices. Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices. * Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes. Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes. 2.4.2. CUSPARSE: RELEASE 12.5\\uf0c1 * New Features Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector. New Features * Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector. Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector. * Resolved Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. Resolved Issues * cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. 2.4.3. CUSPARSE: RELEASE 12.4\\uf0c1 * New Features Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess(). Added support for mixed real and complex types for cusparseSpMM(). Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM(). New Features * Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess(). Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess(). * Added support for mixed real and complex types for cusparseSpMM(). Added support for mixed real and complex types for cusparseSpMM(). * Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM(). Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM(). * Known Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. Known Issues * cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. * Resolved Issues cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros. Resolved Issues * cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros. cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros. 2.4.4. CUSPARSE: RELEASE 12.3 UPDATE 1\\uf0c1 * New Features Added support for block sizes of 64 and 128 in cusparseSDDMM(). Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage. New Features * Added support for block sizes of 64 and 128 in cusparseSDDMM(). Added support for block sizes of 64 and 128 in cusparseSDDMM(). * Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage. Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage. 2.4.5. CUSPARSE: RELEASE 12.3\\uf0c1 * New Features The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector. The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values. New Features * The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector. The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector. * The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values. The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values. * Known Issues The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous. Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A. Known Issues * The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous. The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous. * Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A. Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A. * Resolved Issues cusparseSpSV() provided indeterministic results in some cases. Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment. Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN. Resolved Issues * cusparseSpSV() provided indeterministic results in some cases. cusparseSpSV() provided indeterministic results in some cases. * Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment. Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment. * Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN. Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN. 2.4.6. CUSPARSE: RELEASE 12.2 UPDATE 1\\uf0c1 * New Features The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api. New Features * The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api. The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api. * Resolved Issues Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process. Clarified the supported operations for cusparseSDDMM(). cusparseCreateConstSlicedEll() now uses const pointers. Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing. cusparseSpSM_bufferSize() could ask slightly less memory than needed. cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed. Resolved Issues * Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process. Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process. * Clarified the supported operations for cusparseSDDMM(). Clarified the supported operations for cusparseSDDMM(). * cusparseCreateConstSlicedEll() now uses const pointers. cusparseCreateConstSlicedEll() now uses const pointers. * Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing. Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing.', '* cusparseSpSM_bufferSize() could ask slightly less memory than needed. cusparseSpSM_bufferSize() could ask slightly less memory than needed. * cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed. cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed. * Deprecations Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them. Deprecations * Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them. Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them. 2.4.7. CUSPARSE: RELEASE 12.1 UPDATE 1\\uf0c1 * New Features Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM). Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV). Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step. New Features * Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM). Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM). * Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV). Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV). * Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step. Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step. 2.4.8. CUSPARSE: RELEASE 12.0 UPDATE 1\\uf0c1 * New Features cusparseSDDMM() now supports mixed precision computation. Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs. Improved cusparseSpMV() performance with a new load balancing algorithm. cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address. New Features * cusparseSDDMM() now supports mixed precision computation. cusparseSDDMM() now supports mixed precision computation. * Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs. Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs. * Improved cusparseSpMV() performance with a new load balancing algorithm. Improved cusparseSpMV() performance with a new load balancing algorithm. * cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address. cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address. * Resolved Issues cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows. Resolved Issues * cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows. cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows. 2.4.9. CUSPARSE: RELEASE 12.0\\uf0c1 * New Features JIT LTO functionalities (cusparseSpMMOp()) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan(). Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet(). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions. Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks. Added int8_t support to cusparseGather(), cusparseScatter(), and cusparseCsr2cscEx2(). Improved cusparseSpSV() performance for both the analysis and the solving phases. Improved cusparseSpSM() performance for both the analysis and the solving phases. Improved cusparseSDDMM() performance and added support for batch computation. Improved cusparseCsr2cscEx2() performance. New Features * JIT LTO functionalities (cusparseSpMMOp()) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan(). JIT LTO functionalities (cusparseSpMMOp()) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan(). * Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet(). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions. Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet(). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions. * Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks. Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks. * Added int8_t support to cusparseGather(), cusparseScatter(), and cusparseCsr2cscEx2(). Added int8_t support to cusparseGather(), cusparseScatter(), and cusparseCsr2cscEx2(). * Improved cusparseSpSV() performance for both the analysis and the solving phases. Improved cusparseSpSV() performance for both the analysis and the solving phases. * Improved cusparseSpSM() performance for both the analysis and the solving phases. Improved cusparseSpSM() performance for both the analysis and the solving phases. * Improved cusparseSDDMM() performance and added support for batch computation. Improved cusparseSDDMM() performance and added support for batch computation. * Improved cusparseCsr2cscEx2() performance. Improved cusparseCsr2cscEx2() performance. * Resolved Issues cusparseSpSV() and cusparseSpSM() could produce wrong results. cusparseDnMatGetStridedBatch() did not accept batchStride == 0. Resolved Issues * cusparseSpSV() and cusparseSpSM() could produce wrong results. cusparseSpSV() and cusparseSpSM() could produce wrong results. * cusparseDnMatGetStridedBatch() did not accept batchStride == 0. cusparseDnMatGetStridedBatch() did not accept batchStride == 0. * Deprecations Removed deprecated CUDA 11.x APIs, enumerators, and descriptors. Deprecations * Removed deprecated CUDA 11.x APIs, enumerators, and descriptors. Removed deprecated CUDA 11.x APIs, enumerators, and descriptors. 2.5. MATH LIBRARY\\uf0c1 2.5.1. CUDA MATH: RELEASE 12.5\\uf0c1 * Known Issues As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions. Known Issues * As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions. As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions. 2.5.2. CUDA MATH:']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 79638.68it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 77385.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken to  process each batch & insert data to milvus is:  2.680485725402832\n",
      "====================================================================================================\n",
      "Inside Data embed\n",
      "['RELEASE 12.4\\uf0c1 * Resolved Issues Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules. Resolved Issues * Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules. Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules. 2.5.3. CUDA MATH: RELEASE 12.3\\uf0c1 * New Features Performance of SIMD Integer CUDA Math APIs was improved. New Features * Performance of SIMD Integer CUDA Math APIs was improved. Performance of SIMD Integer CUDA Math APIs was improved. * Resolved Issues The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3. Resolved Issues * The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3. The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3. * Known Issues Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half, __half2, __nv_bfloat16, __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers. Known Issues * Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half, __half2, __nv_bfloat16, __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers. Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half, __half2, __nv_bfloat16, __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers. 2.5.4. CUDA MATH: RELEASE 12.2\\uf0c1 * New Features CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions. __half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release): __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ New Features * CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions. CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions. * __half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release): __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release): * __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ * __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ * Resolved Issues During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware. Updated the observed worst case error bounds for single precision intrinsic functions __expf(), __exp10f() and double precision functions asinh(), acosh(). Resolved Issues * During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware. During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware. * Updated the observed worst case error bounds for single precision intrinsic functions __expf(), __exp10f() and double precision functions asinh(), acosh(). Updated the observed worst case error bounds for single precision intrinsic functions __expf(), __exp10f() and double precision functions asinh(), acosh(). 2.5.5. CUDA MATH: RELEASE 12.1\\uf0c1 * New Features Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf. New Features * Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf. Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf. 2.5.6. CUDA MATH: RELEASE 12.0\\uf0c1 * New Features Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html. New Features * Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html. Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html. * Known Issues Double precision inputs that cause the double precision division algorithm in the default ‘round to nearest even mode’ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected.', 'Affected CUDA Math APIs: __ddiv_rn(). Affected CUDA language operation: double precision / operation in the device code. Known Issues * Double precision inputs that cause the double precision division algorithm in the default ‘round to nearest even mode’ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn(). Affected CUDA language operation: double precision / operation in the device code. Double precision inputs that cause the double precision division algorithm in the default ‘round to nearest even mode’ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn(). Affected CUDA language operation: double precision / operation in the device code. * Deprecations All previously deprecated undocumented APIs are removed from CUDA 12.0. Deprecations * All previously deprecated undocumented APIs are removed from CUDA 12.0. All previously deprecated undocumented APIs are removed from CUDA 12.0. 2.6. NVIDIA PERFORMANCE PRIMITIVES (NPP)\\uf0c1 2.6.1. NPP: RELEASE 12.4\\uf0c1 * New Features Enhanced large file support with size_t. New Features * Enhanced large file support with size_t. Enhanced large file support with size_t. 2.6.2. NPP: RELEASE 12.0\\uf0c1 * Deprecations Deprecating non-CTX API support from next release. Deprecations * Deprecating non-CTX API support from next release. Deprecating non-CTX API support from next release. * Resolved Issues A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance. Resolved Issues * A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance. A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance. 2.7. NVJPEG LIBRARY\\uf0c1 2.7.1. NVJPEG: RELEASE 12.4\\uf0c1 * New Features IDCT performance optimizations for single image CUDA decode. Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE. New Features * IDCT performance optimizations for single image CUDA decode. IDCT performance optimizations for single image CUDA decode. * Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE. Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE. 2.7.2. NVJPEG: RELEASE 12.3 UPDATE 1\\uf0c1 * New Features New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them. New Features * New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them. New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them. 2.7.3. NVJPEG: RELEASE 12.2\\uf0c1 * New Features Added support for JPEG Lossless decode (process 14, FO prediction). nvJPEG is now supported on L4T. New Features * Added support for JPEG Lossless decode (process 14, FO prediction). Added support for JPEG Lossless decode (process 14, FO prediction). * nvJPEG is now supported on L4T. nvJPEG is now supported on L4T. 2.7.4. NVJPEG: RELEASE 12.0\\uf0c1 * New Features Immproved the GPU Memory optimisation for the nvJPEG codec. New Features * Immproved the GPU Memory optimisation for the nvJPEG codec. Immproved the GPU Memory optimisation for the nvJPEG codec. * Resolved Issues An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved. An issue with CMYK four component color conversion is now resolved. Resolved Issues * An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved. An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved. * An issue with CMYK four component color conversion is now resolved. An issue with CMYK four component color conversion is now resolved. * Known Issues Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths. Known Issues * Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths. Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths. * Deprecations The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables). Deprecations * The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables). The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables). Only available on select Linux distros']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 88301.14it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 76260.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken to  process each batch & insert data to milvus is:  2.5525712966918945\n",
      "====================================================================================================\n",
      "overall time to prepare data for insertion:  19.124083518981934\n",
      "Splitting Statistics:\n",
      "  - Total Documents: 44\n",
      "  - Total Splits: 1\n",
      "  - Splits by Threshold: 0\n",
      "  - Splits by Max Chunk Size: 0\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 718\n",
      "  - Maximum Token Size of Split: 718\n",
      "  - Similarity Split Ratio: 0.00\n",
      "overall time to prepare data for insertion:  3.337860107421875e-06\n"
     ]
    }
   ],
   "source": [
    "# Store The Data Into The Milvus\n",
    "message, collection = Store_data_Milvus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [query], <MilvusException: (code=100, message=collection not found[collection=450745645481276887])>, <Time:{'RPC start': '2024-07-15 19:38:00.981352', 'RPC error': '2024-07-15 19:38:01.244442'}>\n"
     ]
    },
    {
     "ename": "MilvusException",
     "evalue": "<MilvusException: (code=100, message=collection not found[collection=450745645481276887])>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMilvusException\u001b[0m                           Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/home/katonic/workspace/Milvus_Feature_Pipeline.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://dev.katonic.ai/home/katonic/workspace/Milvus_Feature_Pipeline.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# to check the data from Present in the Milvus\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev.katonic.ai/home/katonic/workspace/Milvus_Feature_Pipeline.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m collection_query_source \u001b[39m=\u001b[39m collection\u001b[39m.\u001b[39;49mquery(expr\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msource_link like \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps://docs.nvidia.com/cuda\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m, output_fields\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/orm/collection.py:1074\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, expr, output_fields, partition_names, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1073\u001b[0m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[0;32m-> 1074\u001b[0m \u001b[39mreturn\u001b[39;00m conn\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m   1075\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   1076\u001b[0m     expr,\n\u001b[1;32m   1077\u001b[0m     output_fields,\n\u001b[1;32m   1078\u001b[0m     partition_names,\n\u001b[1;32m   1079\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1080\u001b[0m     schema\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_schema_dict,\n\u001b[1;32m   1081\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1082\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:147\u001b[0m, in \u001b[0;36merror_handler.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     LOGGER\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRPC error: [\u001b[39m\u001b[39m{\u001b[39;00minner_name\u001b[39m}\u001b[39;00m\u001b[39m], \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m, <Time:\u001b[39m\u001b[39m{\u001b[39;00mrecord_dict\u001b[39m}\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mFutureTimeoutError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:143\u001b[0m, in \u001b[0;36merror_handler.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     record_dict[\u001b[39m\"\u001b[39m\u001b[39mRPC start\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow())\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m MilvusException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:182\u001b[0m, in \u001b[0;36mtracing_request.<locals>.wrapper.<locals>.handler\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_onetime_request_id(req_id)\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:122\u001b[0m, in \u001b[0;36mretry_on_rpc_failure.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:87\u001b[0m, in \u001b[0;36mretry_on_rpc_failure.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     89\u001b[0m     \u001b[39m# Do not retry on these codes\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py:1503\u001b[0m, in \u001b[0;36mGrpcHandler.query\u001b[0;34m(self, collection_name, expr, output_fields, partition_names, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m-> 1503\u001b[0m check_status(response\u001b[39m.\u001b[39;49mstatus)\n\u001b[1;32m   1505\u001b[0m num_fields \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(response\u001b[39m.\u001b[39mfields_data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/client/utils.py:63\u001b[0m, in \u001b[0;36mcheck_status\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m status\u001b[39m.\u001b[39mcode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m status\u001b[39m.\u001b[39merror_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mraise\u001b[39;00m MilvusException(status\u001b[39m.\u001b[39mcode, status\u001b[39m.\u001b[39mreason, status\u001b[39m.\u001b[39merror_code)\n",
      "\u001b[0;31mMilvusException\u001b[0m: <MilvusException: (code=100, message=collection not found[collection=450745645481276887])>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMilvusException\u001b[0m                           Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/home/katonic/workspace/Milvus_Feature_Pipeline.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://dev.katonic.ai/home/katonic/workspace/Milvus_Feature_Pipeline.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# to check the data from Present in the Milvus\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev.katonic.ai/home/katonic/workspace/Milvus_Feature_Pipeline.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m collection_query_source \u001b[39m=\u001b[39m collection\u001b[39m.\u001b[39;49mquery(expr\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msource_link like \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps://docs.nvidia.com/cuda\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m, output_fields\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/orm/collection.py:1074\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, expr, output_fields, partition_names, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1073\u001b[0m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[0;32m-> 1074\u001b[0m \u001b[39mreturn\u001b[39;00m conn\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m   1075\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   1076\u001b[0m     expr,\n\u001b[1;32m   1077\u001b[0m     output_fields,\n\u001b[1;32m   1078\u001b[0m     partition_names,\n\u001b[1;32m   1079\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1080\u001b[0m     schema\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_schema_dict,\n\u001b[1;32m   1081\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1082\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:147\u001b[0m, in \u001b[0;36merror_handler.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     LOGGER\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRPC error: [\u001b[39m\u001b[39m{\u001b[39;00minner_name\u001b[39m}\u001b[39;00m\u001b[39m], \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m, <Time:\u001b[39m\u001b[39m{\u001b[39;00mrecord_dict\u001b[39m}\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mFutureTimeoutError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:143\u001b[0m, in \u001b[0;36merror_handler.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     record_dict[\u001b[39m\"\u001b[39m\u001b[39mRPC start\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow())\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m MilvusException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:182\u001b[0m, in \u001b[0;36mtracing_request.<locals>.wrapper.<locals>.handler\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_onetime_request_id(req_id)\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:122\u001b[0m, in \u001b[0;36mretry_on_rpc_failure.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:87\u001b[0m, in \u001b[0;36mretry_on_rpc_failure.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     89\u001b[0m     \u001b[39m# Do not retry on these codes\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py:1503\u001b[0m, in \u001b[0;36mGrpcHandler.query\u001b[0;34m(self, collection_name, expr, output_fields, partition_names, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m-> 1503\u001b[0m check_status(response\u001b[39m.\u001b[39;49mstatus)\n\u001b[1;32m   1505\u001b[0m num_fields \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(response\u001b[39m.\u001b[39mfields_data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/client/utils.py:63\u001b[0m, in \u001b[0;36mcheck_status\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m status\u001b[39m.\u001b[39mcode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m status\u001b[39m.\u001b[39merror_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mraise\u001b[39;00m MilvusException(status\u001b[39m.\u001b[39mcode, status\u001b[39m.\u001b[39mreason, status\u001b[39m.\u001b[39merror_code)\n",
      "\u001b[0;31mMilvusException\u001b[0m: <MilvusException: (code=100, message=collection not found[collection=450745645481276887])>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMilvusException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/katonic/workspace/Milvus_Feature_Pipeline.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://dev.katonic.ai/home/katonic/workspace/Milvus_Feature_Pipeline.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# to check the data from Present in the Milvus\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev.katonic.ai/home/katonic/workspace/Milvus_Feature_Pipeline.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m collection_query_source \u001b[39m=\u001b[39m collection\u001b[39m.\u001b[39;49mquery(expr\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msource_link like \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps://docs.nvidia.com/cuda\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m, output_fields\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/orm/collection.py:1074\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, expr, output_fields, partition_names, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[39mraise\u001b[39;00m DataTypeNotMatchException(message\u001b[39m=\u001b[39mExceptionsMessage\u001b[39m.\u001b[39mExprType \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(expr))\n\u001b[1;32m   1073\u001b[0m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[0;32m-> 1074\u001b[0m \u001b[39mreturn\u001b[39;00m conn\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m   1075\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   1076\u001b[0m     expr,\n\u001b[1;32m   1077\u001b[0m     output_fields,\n\u001b[1;32m   1078\u001b[0m     partition_names,\n\u001b[1;32m   1079\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1080\u001b[0m     schema\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_schema_dict,\n\u001b[1;32m   1081\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1082\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:147\u001b[0m, in \u001b[0;36merror_handler.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     record_dict[\u001b[39m\"\u001b[39m\u001b[39mRPC error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow())\n\u001b[1;32m    146\u001b[0m     LOGGER\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRPC error: [\u001b[39m\u001b[39m{\u001b[39;00minner_name\u001b[39m}\u001b[39;00m\u001b[39m], \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m, <Time:\u001b[39m\u001b[39m{\u001b[39;00mrecord_dict\u001b[39m}\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mFutureTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    149\u001b[0m     record_dict[\u001b[39m\"\u001b[39m\u001b[39mgRPC timeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:143\u001b[0m, in \u001b[0;36merror_handler.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     record_dict[\u001b[39m\"\u001b[39m\u001b[39mRPC start\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow())\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m MilvusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    145\u001b[0m     record_dict[\u001b[39m\"\u001b[39m\u001b[39mRPC error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:182\u001b[0m, in \u001b[0;36mtracing_request.<locals>.wrapper.<locals>.handler\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m req_id:\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_onetime_request_id(req_id)\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:122\u001b[0m, in \u001b[0;36mretry_on_rpc_failure.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         back_off \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(back_off \u001b[39m*\u001b[39m back_off_multiplier, max_back_off)\n\u001b[1;32m    121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    124\u001b[0m     \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/decorators.py:87\u001b[0m, in \u001b[0;36mretry_on_rpc_failure.<locals>.wrapper.<locals>.handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     88\u001b[0m     \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     89\u001b[0m         \u001b[39m# Do not retry on these codes\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mcode() \u001b[39min\u001b[39;00m IGNORE_RETRY_CODES:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py:1503\u001b[0m, in \u001b[0;36mGrpcHandler.query\u001b[0;34m(self, collection_name, expr, output_fields, partition_names, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[39mif\u001b[39;00m Status\u001b[39m.\u001b[39mEMPTY_COLLECTION \u001b[39min\u001b[39;00m {response\u001b[39m.\u001b[39mstatus\u001b[39m.\u001b[39mcode, response\u001b[39m.\u001b[39mstatus\u001b[39m.\u001b[39merror_code}:\n\u001b[1;32m   1502\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m-> 1503\u001b[0m check_status(response\u001b[39m.\u001b[39;49mstatus)\n\u001b[1;32m   1505\u001b[0m num_fields \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(response\u001b[39m.\u001b[39mfields_data)\n\u001b[1;32m   1506\u001b[0m \u001b[39m# check has fields\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pymilvus/client/utils.py:63\u001b[0m, in \u001b[0;36mcheck_status\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_status\u001b[39m(status: Status):\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m status\u001b[39m.\u001b[39mcode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m status\u001b[39m.\u001b[39merror_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[39mraise\u001b[39;00m MilvusException(status\u001b[39m.\u001b[39mcode, status\u001b[39m.\u001b[39mreason, status\u001b[39m.\u001b[39merror_code)\n",
      "\u001b[0;31mMilvusException\u001b[0m: <MilvusException: (code=100, message=collection not found[collection=450745645481276887])>"
     ]
    }
   ],
   "source": [
    "# to check the data from Present in the Milvus\n",
    "collection_query_source = collection.query(expr='source_link like \"https://docs.nvidia.com/cuda%\"', output_fields=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: [] , extra_info: {'cost': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_query_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
